# Code completion

The project's focus is on code completion. The pipeline separates code samples into three sections: prefix (before the cursor), middle (missing code), and suffix (after the cursor). 
The middle section's completions are generated using pre-trained open-source models: `tiny_starcoder_py`, `starcoder2_3b`, `starcoder2_7b`, and `starcoder2_15b`.

After creating completions, the outputs are examined both manually and automatically using the target code.

The goal is to establish which metrics correspond best with human evaluation and to assess the quality of code completion across various models.

## Steps of solution
0. **Exploring the models**:
* Explore different models ([explore_tiny_starcoder.ipynb](https://github.com/TyKo0707/code_completion/blob/main/models_explore/explore_tiny_starcoder.ipynb), [explore_big_starcoder.ipynb](https://github.com/TyKo0707/code_completion/blob/main/models_explore/explore_big_starcoder.ipynb)) specified for code generation to find the most suitable for the task.
* Find the optimal config and method of generation for each model.
* Result of this step: decided to use starcoder and starcoder-based models `tiny_starcoder_py`, `starcoder2_3b`, `starcoder2_7b`, and `starcoder2_15b`.


1. **Data Collection and Preprocessing**:<br>
I need to create a dataset consists of prefix, middle part (target), and suffix.
* Took a few files ([data/python_lang](https://github.com/TyKo0707/code_completion/tree/main/data/python_lang)) from the local repository and annotated them. 
Each target is located inside `$$tag ...$$` symbols. 
There are 9 different types of tags (`code_by_description, conditional_statement, var_declaration, class_initialization, function_name, function_parameter, description_by_code, method_call, imports`) that can be later used for troubleshooting or analysis, but as of now it is just a small bonus since the dataset is relatively small. 
Example:
```python 
tensorboard_callback = keras.callbacks.$$method_call TensorBoard(log_dir=logdir, histogram_freq=1)$$
# target part is `TensorBoard(log_dir=logdir, histogram_freq=1)` with tag method_call

def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return $$code_by_description datetime.now().strftime("%Y%m%d%H%M%S%f")$$
    # target part is `code_by_description datetime.now().strftime("%Y%m%d%H%M%S%f")` with tag code_by_description
```
* Extracted all targets using regex along with suffixes and prefixes from the same file. The resulting dataset consists of 37 code snippets and can be found here: [data/python_dataset.csv](https://github.com/TyKo0707/code_completion/blob/main/data/python_dataset.csv).


2. **Generation Code Completions**:* Using dataset and 4 models, generate missing code parts and save them into [data/python_dataset_gen.csv](https://github.com/TyKo0707/code_completion/blob/main/data/python_dataset_gen.csv)


3. **Evaluation**:<br> 
The most exciting part is here. I need to evaluate generated data manually and automatically and then compare different metrics by task suitability.
* If I have a generated sample and its desired form (target) along with context (prefix and suffix), I want to measure:
  * **Exact match**: If generated code is equal to target, we can skip all following parts of measuring.
  * **Functional correctness**: Does the code generated by the model work?
  * **Syntactic similarity**: How close the code is to the target syntactically?
  * **Semantic similarity**: How well the model understands the context (maybe the model generates slightly different code from the original syntactically, but it works and the task is accomplished).
* Taking all of this into account, I came up with the following manual metrics:
  * **functional_correctness**: Measures whether code is valid and will compile.
  * **factual_correctness**: Measures whether code solved the actual task.
  * **relevance**: How well did the model understand the task and context of the missing part? The least objective metric, but still a required metric.
* Same logic for the automatic metrics. In parentheses is written the class to which I refer to the metric:
  * **exact_match**	(Exact match): Checks if the generated code is identical to the reference.
  * **chrf3** (Syntactic similarity): Evaluates similarity at the character level, capturing fine-grained details and being robust to variations in word boundaries and morphology.
  * **edit_distance** (Syntactic similarity): Tracks the number of changes needed to match the reference.
  * **embedding_similarity** (Semantic similarity): Compares semantic meaning between generated and reference code using word embeddings trained for code: [codet5p-110m-embedding](https://huggingface.co/Salesforce/codet5p-110m-embedding).
  * **rouge_l**	(Syntactic similarity): Measures the longest common subsequence between the generated and reference code, focusing on token or substring overlap.
  * **function_correctness_llm** (Functional correctness): This is a specialized metric to automatically check whether the generated code can execute without errors. I am using LLMs here instead of writing tests for each sample because it works just as effectively but is much more efficient to use.