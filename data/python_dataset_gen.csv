,prefix,tag,content,suffix,file_name,gen_tiny_starcoder_py,gen_starcoder2_3b,gen_starcoder2_7b,gen_starcoder2_15b
0,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return ",code_by_description,"datetime.now().strftime(""%Y%m%d%H%M%S%f"")","


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''
        Logs the actions taken by the agent during a game round.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt,"datetime.now().strftime(""%Y%m%d_%H%M%S"")",datetime.now().strftime('%Y%m%d%H%M%S%f'),"datetime.now().strftime(""%Y%m%d%H%M%S%f"")[:-3]","datetime.now().strftime(""%Y%m%d%H%M%S%f"")[:-3]"
1,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    ",code_by_description,"assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')","
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''
        Logs the actions taken by the agent during a game round.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt,"message = message.split(""<s>"")","assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')","assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')","assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')"
2,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    ",code_by_description,"assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices","
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''
        Logs the actions taken by the agent during a game round.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt,if '<s>' in message and '</s>' in message:,"assert '<s>' not in choices and '</s>' not in choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')","assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')","assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>', start)"
3,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == ",conditional_statement,'default',":
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''
        Logs the actions taken by the agent during a game round.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt,'defect':,"'default':
            action, reason = self.make_default_action()
        elif self.strategy == 'open source':
            action, reason = self.make_open_source_action()
        elif self.strategy == 'closed source':
            action, reason = self.make_closed_source_action()
        else",'default','default'
4,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = ",var_declaration,"random.choice(['choice_1', 'choice_2']), 'random choice'","
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''
        Logs the actions taken by the agent during a game round.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt,"_AGENT_CLASSES = [Agent, Game]","random.choices(['choice_1', 'choice_2'])[0], 'randomly picked'","random.sample(['choice_1', 'choice_2'], k=1)[0], 'I don\'t care.'","random.choices(['choice_1', 'choice_2'], weights=[5, 3])[0], 'randomly choose an option'"
5,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = ",var_declaration,"'choice_2', 'always defect'","
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''
        Logs the actions taken by the agent during a game round.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt,"'choice_1', 'defect'","'choice_2','my first move is always defect'","'choice_2', 'always defect'","'choice_2', 'always defect'
                case 'titfortat':
                    action, reason = 'choice_1', 'always cooperate'"
6,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = ",var_declaration,"'choice_1', 'always cooperate'","
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''
        Logs the actions taken by the agent during a game round.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt,"'choice_1', 'always cooperate'","'choice_1','my first move is always cooperate'","'choice_1','stay cooperative'","'choice_1', 'always cooperate'
                case 'titfortat':
                    if len(self.previous_message) == 0:
                        action, reason = 'choice_1', 'first move is always cooperation'
                    elif self.previous_message[-1].split()[3] == 'choice_1':
                        action, reason = 'choice_1', 'if my opponent played cooperation last turn then I will also do so.'
                    else:
                        action, reason = 'choice_2', 'if my opponent played defection last turn then I will also do so.'"
7,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        ",class_initialization,"self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled","

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''
        Logs the actions taken by the agent during a game round.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt,self.agents = agents,"self.agents = agents
        self.game_type = game_type","'''
        Initializes the game object.
        :param agents: List of two agents participating in the game.
        :param game_type: Type of the game being played ('ovo','mpo', etc.)
        '''
        self.agents = agents
        self.game_type = game_type","self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled  # Flag to enable/disable logging"
8,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def ",function_name,log_action,"(self, agent, roundn, action, opp_action, reason, reward):
        '''
        Logs the actions taken by the agent during a game round.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt,__str__(self):,log_action,log_action,log_action
9,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, ",function_parameter,"agent, roundn, action, opp_action, reason, reward","):
        '''
        Logs the actions taken by the agent during a game round.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt,"action, roundn, agent_action, agent_reason, reward","agent, roundn, action, opp_action, reason, reward","agent, roundn, action, opp_action, reason, reward","agent, roundn, action, opp_action, reason, reward"
10,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''
        ",description_by_code,Logs the actions taken by the agent during a game round.,"
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt,This method logs an action that is taken by the agent. The agent will then choose one of the following actions:,Logs the actions taken by an agent during one round,Logs an agent's actions and rewards,"Logs an action taken by an agent during a round of the game.
        :param agent: The agent who took the action.
        :param roundn: The number of the round being played.
        :param action: The action taken by the agent.
        :param opp_action: The action taken by the other player.
        :param reason: The reason provided by the agent for their decision.
        :param reward: The rewards received by both players after taking actions."
11,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''
        Logs the actions taken by the agent during a game round.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.",method_call,log_action,"(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt,"play_one_round(roundn, alice_action, bob_action, alice_reason, reward)",log_action,log_action,log_action
12,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''
        Logs the actions taken by the agent during a game round.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.",method_call,play_single_round_ovo(i),"
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt,play_single_round_ovo(i),play_single_round_ovo(i),play_single_round_ovo(i),play_single_round_ovo(i)
13,"import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt


def extract_base_function(true_function):
    return true_function.split('.')[-1]


def ",function_name,check_function_in_generated,"(base_function, generated_code, log=False):
    try:
        possible_functions = re.findall(r'\b\w+\b', generated_code)
        results = base_function in possible_functions
        if log:
            print(f'True: \033[96m{base_function}\033[00m. \nGenerated: \033[96m{generated_code.strip()}\033[00m. '
                  f'\nResults: \033[96m{results}\033[00m\n')
        return results
    except:
        return False


# Define the models and read their datasets
num_samples = 3000
dfs = {
    'CodeStral-22B': pd.read_csv('../../data/generation/generation_results_codelstral_22B.csv')[:num_samples],
    'Mistral-7B': pd.read_csv('../../data/generation/generation_results_mistral_7B.csv')[:num_samples],
    'Phi-2': pd.read_csv('../../data/generation/generation_results_phi-2.csv')[:num_samples],
    'CodeGen2-3_7B': pd.read_csv('../../data/generation/generation_results_codegen2-3_7B.csv')[:num_samples],
    'CodeGen25-7B-Mono': pd.read_csv('../../data/generation/generation_results_codegen25-7b-monoB.csv')[:num_samples],
}

important_sections = ['DataFrame', 'General functions', 'GroupBy', 'Index objects', 'Input/output', 'Series']

overall_ratios = []
section_ratios_list = []

# Iterate through each model and process the data
for name, df in dfs.items():
    df = df[df['section_name'].isin(important_sections)]

    df['match_found'] = df.apply(
        lambda row: check_function_in_generated(extract_base_function(row['function_name']), row['generation']), axis=1)
    df_false = df[df.match_found == False]
    df_false = df_false[['function_name', 'description', 'section_name', 'subsection_name']]
    df_false.to_csv(f'{name}_false_gen.csv')
    print(len(df_false))
    overall_ratio = df.match_found.sum() / len(df)
    overall_ratios.append({'Model': name, 'Overall Correctness Ratio': overall_ratio})

    # Calculate the correctness ratio for each section
    section_ratios = df.groupby('section_name')['match_found'].mean().reset_index()
    section_ratios = section_ratios.rename(columns={'match_found': 'correctness_ratio'})
    section_counts = df.groupby('section_name').size().reset_index(name='row_count')
    section_ratios = pd.merge(section_ratios, section_counts, on='section_name')

    print(section_ratios.correctness_ratio.var())

    print(f'\nResults for {name} model:\nOverall correctness ratio: {overall_ratio:.4f}\n{section_ratios}')

    section_ratios['Model'] = name
    section_ratios_list.append(section_ratios)

# Combine section ratios for all models into a single DataFrame
section_ratios_df = pd.concat(section_ratios_list, ignore_index=True)
overall_ratios_df = pd.DataFrame(overall_ratios)

# Set up plotting style
sns.set(style=""whitegrid"")

# Plot 1: Overall Correctness Ratio for Each Model
plt.figure(figsize=(12, 8))
sns.barplot(x='Model', y='Overall Correctness Ratio', data=overall_ratios_df, palette=""muted"")
plt.title('Overall Correctness Ratio by Model')
plt.ylabel('Correctness Ratio')
plt.xlabel('Model')
plt.ylim(0, 1)
plt.savefig('../../data/plots/correctness_models.png', dpi=300)

# Plot 2: Correctness Ratio by Section for Each Model
plt.figure(figsize=(16, 9))
sns.barplot(x='correctness_ratio', y='section_name', hue='Model', data=section_ratios_df, palette=""muted"")
plt.title('Correctness Ratio by Section and Model')
plt.xlabel('Correctness Ratio')
plt.ylabel('Section Name')
plt.xlim(0, 1)
plt.legend(title='Model')
plt.savefig('../../data/plots/correctness_models_by_section.png', dpi=300)

plt.show()

",generation_eda.txt,check_function_in_generated,check_function_in_generated,check_function_in_generated,"check_function_in_generated(base_function, generated_code, log=False):"
14,"import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt


def extract_base_function(true_function):
    return true_function.split('.')[-1]


def check_function_in_generated(base_function, generated_code, ",function_parameter,log=False,"):
    try:
        possible_functions = re.findall(r'\b\w+\b', generated_code)
        results = base_function in possible_functions
        if log:
            print(f'True: \033[96m{base_function}\033[00m. \nGenerated: \033[96m{generated_code.strip()}\033[00m. '
                  f'\nResults: \033[96m{results}\033[00m\n')
        return results
    except:
        return False


# Define the models and read their datasets
num_samples = 3000
dfs = {
    'CodeStral-22B': pd.read_csv('../../data/generation/generation_results_codelstral_22B.csv')[:num_samples],
    'Mistral-7B': pd.read_csv('../../data/generation/generation_results_mistral_7B.csv')[:num_samples],
    'Phi-2': pd.read_csv('../../data/generation/generation_results_phi-2.csv')[:num_samples],
    'CodeGen2-3_7B': pd.read_csv('../../data/generation/generation_results_codegen2-3_7B.csv')[:num_samples],
    'CodeGen25-7B-Mono': pd.read_csv('../../data/generation/generation_results_codegen25-7b-monoB.csv')[:num_samples],
}

important_sections = ['DataFrame', 'General functions', 'GroupBy', 'Index objects', 'Input/output', 'Series']

overall_ratios = []
section_ratios_list = []

# Iterate through each model and process the data
for name, df in dfs.items():
    df = df[df['section_name'].isin(important_sections)]

    df['match_found'] = df.apply(
        lambda row: check_function_in_generated(extract_base_function(row['function_name']), row['generation']), axis=1)
    df_false = df[df.match_found == False]
    df_false = df_false[['function_name', 'description', 'section_name', 'subsection_name']]
    df_false.to_csv(f'{name}_false_gen.csv')
    print(len(df_false))
    overall_ratio = df.match_found.sum() / len(df)
    overall_ratios.append({'Model': name, 'Overall Correctness Ratio': overall_ratio})

    # Calculate the correctness ratio for each section
    section_ratios = df.groupby('section_name')['match_found'].mean().reset_index()
    section_ratios = section_ratios.rename(columns={'match_found': 'correctness_ratio'})
    section_counts = df.groupby('section_name').size().reset_index(name='row_count')
    section_ratios = pd.merge(section_ratios, section_counts, on='section_name')

    print(section_ratios.correctness_ratio.var())

    print(f'\nResults for {name} model:\nOverall correctness ratio: {overall_ratio:.4f}\n{section_ratios}')

    section_ratios['Model'] = name
    section_ratios_list.append(section_ratios)

# Combine section ratios for all models into a single DataFrame
section_ratios_df = pd.concat(section_ratios_list, ignore_index=True)
overall_ratios_df = pd.DataFrame(overall_ratios)

# Set up plotting style
sns.set(style=""whitegrid"")

# Plot 1: Overall Correctness Ratio for Each Model
plt.figure(figsize=(12, 8))
sns.barplot(x='Model', y='Overall Correctness Ratio', data=overall_ratios_df, palette=""muted"")
plt.title('Overall Correctness Ratio by Model')
plt.ylabel('Correctness Ratio')
plt.xlabel('Model')
plt.ylim(0, 1)
plt.savefig('../../data/plots/correctness_models.png', dpi=300)

# Plot 2: Correctness Ratio by Section for Each Model
plt.figure(figsize=(16, 9))
sns.barplot(x='correctness_ratio', y='section_name', hue='Model', data=section_ratios_df, palette=""muted"")
plt.title('Correctness Ratio by Section and Model')
plt.xlabel('Correctness Ratio')
plt.ylabel('Section Name')
plt.xlim(0, 1)
plt.legend(title='Model')
plt.savefig('../../data/plots/correctness_models_by_section.png', dpi=300)

plt.show()

",generation_eda.txt,log,log,log,log=False
15,"import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt


def extract_base_function(true_function):
    return true_function.split('.')[-1]


def check_function_in_generated(base_function, generated_code, log=False):
    try:
        possible_functions = re.findall(r'\b\w+\b', generated_code)
        results = base_function in possible_functions
        if ",conditional_statement,log,":
            print(f'True: \033[96m{base_function}\033[00m. \nGenerated: \033[96m{generated_code.strip()}\033[00m. '
                  f'\nResults: \033[96m{results}\033[00m\n')
        return results
    except:
        return False


# Define the models and read their datasets
num_samples = 3000
dfs = {
    'CodeStral-22B': pd.read_csv('../../data/generation/generation_results_codelstral_22B.csv')[:num_samples],
    'Mistral-7B': pd.read_csv('../../data/generation/generation_results_mistral_7B.csv')[:num_samples],
    'Phi-2': pd.read_csv('../../data/generation/generation_results_phi-2.csv')[:num_samples],
    'CodeGen2-3_7B': pd.read_csv('../../data/generation/generation_results_codegen2-3_7B.csv')[:num_samples],
    'CodeGen25-7B-Mono': pd.read_csv('../../data/generation/generation_results_codegen25-7b-monoB.csv')[:num_samples],
}

important_sections = ['DataFrame', 'General functions', 'GroupBy', 'Index objects', 'Input/output', 'Series']

overall_ratios = []
section_ratios_list = []

# Iterate through each model and process the data
for name, df in dfs.items():
    df = df[df['section_name'].isin(important_sections)]

    df['match_found'] = df.apply(
        lambda row: check_function_in_generated(extract_base_function(row['function_name']), row['generation']), axis=1)
    df_false = df[df.match_found == False]
    df_false = df_false[['function_name', 'description', 'section_name', 'subsection_name']]
    df_false.to_csv(f'{name}_false_gen.csv')
    print(len(df_false))
    overall_ratio = df.match_found.sum() / len(df)
    overall_ratios.append({'Model': name, 'Overall Correctness Ratio': overall_ratio})

    # Calculate the correctness ratio for each section
    section_ratios = df.groupby('section_name')['match_found'].mean().reset_index()
    section_ratios = section_ratios.rename(columns={'match_found': 'correctness_ratio'})
    section_counts = df.groupby('section_name').size().reset_index(name='row_count')
    section_ratios = pd.merge(section_ratios, section_counts, on='section_name')

    print(section_ratios.correctness_ratio.var())

    print(f'\nResults for {name} model:\nOverall correctness ratio: {overall_ratio:.4f}\n{section_ratios}')

    section_ratios['Model'] = name
    section_ratios_list.append(section_ratios)

# Combine section ratios for all models into a single DataFrame
section_ratios_df = pd.concat(section_ratios_list, ignore_index=True)
overall_ratios_df = pd.DataFrame(overall_ratios)

# Set up plotting style
sns.set(style=""whitegrid"")

# Plot 1: Overall Correctness Ratio for Each Model
plt.figure(figsize=(12, 8))
sns.barplot(x='Model', y='Overall Correctness Ratio', data=overall_ratios_df, palette=""muted"")
plt.title('Overall Correctness Ratio by Model')
plt.ylabel('Correctness Ratio')
plt.xlabel('Model')
plt.ylim(0, 1)
plt.savefig('../../data/plots/correctness_models.png', dpi=300)

# Plot 2: Correctness Ratio by Section for Each Model
plt.figure(figsize=(16, 9))
sns.barplot(x='correctness_ratio', y='section_name', hue='Model', data=section_ratios_df, palette=""muted"")
plt.title('Correctness Ratio by Section and Model')
plt.xlabel('Correctness Ratio')
plt.ylabel('Section Name')
plt.xlim(0, 1)
plt.legend(title='Model')
plt.savefig('../../data/plots/correctness_models_by_section.png', dpi=300)

plt.show()

",generation_eda.txt,possible_functions,log,log,log
16,from transformers import ,imports,"AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig","
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""
    Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt,AutoModelForCausalLM,"AutoTokenizer
from transformers import BitsAndBytesConfig
from transformers import DataCollatorForLanguageModeling","AutoTokenizer, AutoModelForCausalLM","AutoTokenizer, AutoModelForCausalLM"
17,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = ",var_declaration,datetime.datetime.now(),"

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""
    Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt,"time.strftime(""%Y-%m-%d %H:%M:%S"")",datetime.datetime.now(),datetime.datetime.now(),"datetime.datetime.now()
os.environ[""TOKENIZERS_PARALLELISM""] = ""false"""
18,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = ",var_declaration,"BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16","
)

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""
    Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt,BitAndBytesConfig(,"BitsAndBytesConfig(
    load_in_8bit=True","BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_4bit_quant_type=""nf4"",","BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True"
19,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.",method_call,"from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,","
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""
    Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt,"Dataset.from_pretrained('microsoft/phi-1_5', use_fast=True).eval().cuda()","from_pretrained('microsoft/phi-1_5', trust_remote_code=True).half().cuda() #.half()","from_pretrained(
    ""microsoft/phi-1_5"",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16","from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float16, device_map='auto', bnb_config=bnb_config"
20,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""
    Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:",description_by_code,"        code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.","
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt,,"
        code (str): The input code string that may contain special tags.

    Returns:
        str: The modified version of the original code without any special tags.","
        code (str): The input code string.

    Returns:
        str: The processed code string after replacing special tags with their corresponding values.","
        code (str): The input code string.

    Returns:
        str: The updated code string after replacing all special tags."
21,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""
    Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    ",code_by_description,"data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)","
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt,"return [json.loads(line)[""signature""].strip(), json.loads(line)[""body""].strip()]",data = [],"with open(file_path, encoding='utf8') as f:
        data = [json.loads(line) for line in f if len(line)!= 0]","with open(file_path, 'rb') as f:
        lines = [line.decode('utf8').strip('\n') for line in f.readlines()]
    data = []
    for line in lines:
        obj = json.loads(line)
        if'signature' in obj:
            obj['signature'] = replace_tags(obj['signature'])
        if 'body' in obj:
            obj['body'] = replace_tags(obj['body'])
        data.append(obj)"
22,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""
    Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = ",method_call,read_jsonl_file(file_path),"
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt,read_jsonl_file(file_path),read_jsonl_file(file_path)[987:],read_jsonl_file(file_path),read_jsonl_file(file_path)
23,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""
    Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.",method_call,read_csv(file_path),"
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt,load_csv(file_path),read_csv(file_path),read_csv(file_path),read_csv(file_path)
24,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""
    Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(",method_call,"sample[""text""], padding=True, truncation=True, max_length=256",")
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt,"sample['text'], padding='max_length', truncation=True, return_tensors='pt').input_ids","sample[""text""])
    if sample is not None:
      tokenized_text[""input_ids""].extend([[tokenizer.bos_token_id]]) # Add start of sequence token
      tokenized_text[""attention_mask""].extend([1]*len(tokenized_text[""input_ids""])) # Mask all tokens except the first one
      tokenized_text[""labels""].extend([-1]+list(tokenized_text[""input_ids""][1:])","sample[""text""])
    tokenized_text[""labels""] = tokenized_text[""input_ids""].copy(","sample[""text""])
    sample[""input_ids""] = tokenized_text[""input_ids""][0]
    sample[""attention_mask""] = tokenized_text[""attention_mask""][0"
25,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""
    Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = ",var_declaration,"Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)","
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt,Trainer(,"Trainer(
    model=model,
    args=training_arguments,
    train_dataset=tokenized_data","Trainer(
    model=model,
    args=training_arguments,
    train_dataset=tokenized_data","Trainer(
    model=model,
    args=training_arguments,
    train_dataset=tokenized_data,
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer)"
26,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import ",imports,"Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, concatenate","
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt,"BatchNormalization, Activation, GlobalAveragePooling2D","Input, Dense, Flatten, Dropout, MaxPool2D, \","Input, Flatten, Dense, Dropout, Lambda, Concatenate, Add, Multiply
from keras.layers import Conv2D, MaxPool2D, ZeroPadding2D, AveragePooling2D, GlobalAveragePooling2D, BatchNormalization
from keras.layers import Activation, DepthwiseConv2D, ReLU, Softmax
from keras.optimizers import Adam","Activation, Add, AveragePooling2D, BatchNormalization, Concatenate, \"
27,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            ",class_initialization,"self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])","

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt,self._data = data,"self.__dict__.update({key: value[:size].copy() for key, value in data.items()})","self._data = {}
            self._size = sum(map(lambda x: x.shape[0], data.values()))",self._data = {}
28,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    ",code_by_description,"x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)","
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt,"return BatchNormalization()(BatchNormalization()(Conv2D(filters, kernel_size, strides, padding=padding)))(x)",x = BatchNormalization()(x),"x = Conv2D(filters, kernel_size, strides=strides, padding=padding)(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)",x = BatchNormalization()(x)
29,"import datetime
import numpy as np
import os
import sys
import urllib.request
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(",function_parameter,"input_shape, num_classes",", growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt,num_classes,"input_shape, num_classes",input_shape,"input_shape, num_classes"
30,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = ",var_declaration,"transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
]",")


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt,transforms.Compose([,transforms.Compose([,transforms.Compose([,transforms.Compose([
31,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, ",function_parameter,batch_size,"):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt,batch_size,batch_size,batch_size,batch_size
32,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        ",class_initialization,"self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf","

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt,self.filepath = filepath,self.filepath = filepath,"self.monitor = monitor
        self.mode = mode
        self.filepath = filepath
        self.threshold = threshold
        self.best = float('-inf')",self.monitor = monitor
33,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = ",var_declaration,"create_densenet((32, 32, 3), 10)","
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt,"create_densenet((32, 32, 3), num_classes=10, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5)","create_densenet((CIFAR10.H, CIFAR10.W, CIFAR10.C),
                         num_classes=len(CIFAR10.LABELS),
                         growth_rate=12,
                         block_layers=[6, 12, 24, 16],
                         compression_factor=0.7)","create_densenet((CIFAR10.H, CIFAR10.W, CIFAR10.C), num_classes=10)","create_densenet((32, 32, 3), 10, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5)"
34,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.",method_call,"TensorBoard(log_dir=logdir, histogram_freq=1)","
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt,"TensorBoard(log_dir=logdir, histogram_freq=0, write_graph=False, update_freq=""epoch"")","TensorBoard(log_dir=logdir, histogram_freq=1)","TensorBoard(log_dir=logdir, histogram_freq=1, write_graph=False, update_freq=""epoch"")","TensorBoard(log_dir=logdir, histogram_freq=1, write_graph=False, update_freq=""epoch"")"
35,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = ",var_declaration,"int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))","
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt,10,int(np.ceil(cifar.train.size / float(args.batch_size))),int(np.ceil(len(cifar.train.data['images']) / float(args.batch_size))),int(np.ceil(cifar.train.size / float(args.batch_size)))
36,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.",method_call,fit,"(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt,build,fit,fit,fit
