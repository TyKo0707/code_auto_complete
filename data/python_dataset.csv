,prefix,tag,content,suffix,file_name
0,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    ",code_by_description,"return datetime.now().strftime(""%Y%m%d%H%M%S%f"")","


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''Logs the actions taken by the agent during a game round.

        This function constructs a log message that includes the round number,
        the agent's action, the opponent's action, the rewards received,
        and the reasoning behind the chosen action. If logging is enabled,
        this message is appended to the agent's previous messages and reasons.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt
1,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    ",code_by_description,"assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')","
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''Logs the actions taken by the agent during a game round.

        This function constructs a log message that includes the round number,
        the agent's action, the opponent's action, the rewards received,
        and the reasoning behind the chosen action. If logging is enabled,
        this message is appended to the agent's previous messages and reasons.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt
2,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    ",code_by_description,"assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices","
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''Logs the actions taken by the agent during a game round.

        This function constructs a log message that includes the round number,
        the agent's action, the opponent's action, the rewards received,
        and the reasoning behind the chosen action. If logging is enabled,
        this message is appended to the agent's previous messages and reasons.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt
3,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy ",conditional_statement,== 'default',":
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''Logs the actions taken by the agent during a game round.

        This function constructs a log message that includes the round number,
        the agent's action, the opponent's action, the rewards received,
        and the reasoning behind the chosen action. If logging is enabled,
        this message is appended to the agent's previous messages and reasons.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt
4,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = ",var_declaration,"random.choice(['choice_1', 'choice_2']), 'random choice'","
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''Logs the actions taken by the agent during a game round.

        This function constructs a log message that includes the round number,
        the agent's action, the opponent's action, the rewards received,
        and the reasoning behind the chosen action. If logging is enabled,
        this message is appended to the agent's previous messages and reasons.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt
5,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = ",var_declaration,"'choice_2', 'always defect'","
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''Logs the actions taken by the agent during a game round.

        This function constructs a log message that includes the round number,
        the agent's action, the opponent's action, the rewards received,
        and the reasoning behind the chosen action. If logging is enabled,
        this message is appended to the agent's previous messages and reasons.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt
6,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = ",var_declaration,"'choice_1', 'always cooperate'","
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''Logs the actions taken by the agent during a game round.

        This function constructs a log message that includes the round number,
        the agent's action, the opponent's action, the rewards received,
        and the reasoning behind the chosen action. If logging is enabled,
        this message is appended to the agent's previous messages and reasons.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt
7,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        ",class_initialization,"self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled","

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''Logs the actions taken by the agent during a game round.

        This function constructs a log message that includes the round number,
        the agent's action, the opponent's action, the rewards received,
        and the reasoning behind the chosen action. If logging is enabled,
        this message is appended to the agent's previous messages and reasons.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt
8,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def ",function_name,log_action,"(self, agent, roundn, action, opp_action, reason, reward):
        '''Logs the actions taken by the agent during a game round.

        This function constructs a log message that includes the round number,
        the agent's action, the opponent's action, the rewards received,
        and the reasoning behind the chosen action. If logging is enabled,
        this message is appended to the agent's previous messages and reasons.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt
9,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, ",function_parameter,"agent, roundn, action, opp_action, reason, reward","):
        '''Logs the actions taken by the agent during a game round.

        This function constructs a log message that includes the round number,
        the agent's action, the opponent's action, the rewards received,
        and the reasoning behind the chosen action. If logging is enabled,
        this message is appended to the agent's previous messages and reasons.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt
10,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''",description_by_code,"        Logs the actions taken by the agent during a game round.

        This function constructs a log message that includes the round number,
        the agent's action, the opponent's action, the rewards received,
        and the reasoning behind the chosen action. If logging is enabled,
        this message is appended to the agent's previous messages and reasons.","
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt
11,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''Logs the actions taken by the agent during a game round.

        This function constructs a log message that includes the round number,
        the agent's action, the opponent's action, the rewards received,
        and the reasoning behind the chosen action. If logging is enabled,
        this message is appended to the agent's previous messages and reasons.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.",method_call,log_action,"(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.play_single_round_ovo(i)
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt
12,"import argparse
from rules import create_rules, create_cot_prompt
import models
from loguru import logger
import time
import random
import problems_config as pcfg
from datetime import datetime


def generate_time_based_id():
    # Get the current time in the format YYYYMMDDHHMMSSFFF (year, month, day, hour, minute, second, millisecond)
    return datetime.now().strftime(""%Y%m%d%H%M%S%f"")


def parse(message):
    # Extracts the substring between <s> and </s> tags in the given message
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    return message[start:end]


def parse_action(message, choices):
    # Extracts the action between <s> and </s> tags and ensures it's in the list of choices
    assert '<s>' in message and '</s>' in message
    start = message.index('<s>') + len('<s>')
    end = message.index('</s>')
    action = message[start:end].strip('\n').strip()
    assert action in choices
    return message[start:end], message[end + 4:].strip()


# ========== Agent Class Definition ==========
class Agent:
    def __init__(self, args, names, payoffs, strategy='default', context=0, log_enabled=True):
        self.args = args
        self.name = names[0]
        self.the_other_player = names[1]
        self.previous_message, self.previous_reasons = [], []
        self.payoffs = payoffs
        self.strategy = strategy
        self._game_setting, self._action_prompt = create_rules(names, payoffs, strategy)
        self.context = context
        self.log_enabled = log_enabled  # Flag to enable/disable logging

        if self.log_enabled:
            logger.info(f""Initialized {self.name} with strategy {self.strategy} and context {self.context}"")

    # ========== Method to Decide Agent's Action ==========
    def make_action(self):
        action, reason = '', ''
        if self.strategy == 'default':
            action_prompt = self._action_prompt
            if self.context > 0 and len(self.previous_message) > 0:
                previous_messages = f'Results of previous {self.context} round(s): ' \
                                    f'\n{"" "".join(self.previous_message[-self.context:])}'
                action_prompt = previous_messages + '\n' + action_prompt

            cot_prompt = create_cot_prompt(None if len(self.previous_reasons) == 0 else self.previous_reasons[-1],
                                           'single' if self.context == 1 else 'multi')

            action_prompt = cot_prompt + action_prompt + 'Don\'t forget to follow your strategy. ' \
                                                         'So, your answer should be in form of: <s>choice_</s> Reason: ...'

            # Avoid repetition of reasons to prevent the agent from getting stuck
            if len(self.previous_reasons) > 1:
                if self.previous_reasons[-1] == self.previous_reasons[-2]:
                    action_prompt += 'Review your previous reason and look at the rules of the game. ' \
                                  'You probably need to change something.\n'

            action_prompt = self._game_setting + '\n' + action_prompt
            while True:
                try:
                    action_message = models.close_source_call('gemini', action_prompt, self.args.system_prompt)
                    action, reason = parse_action(action_message, list(self.payoffs.keys()))
                    reason = reason.replace('\n', ' ')
                    if self.log_enabled:
                        logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
                    return action, reason
                except:
                    if self.log_enabled:
                        logger.error(f""Unable to call the model."")
                    time.sleep(0.1)
        else:
            # Handle different strategies
            match self.strategy:
                case 'random':
                    action, reason = random.choice(['choice_1', 'choice_2']), 'random choice'
                case 'defect':
                    action, reason = 'choice_2', 'always defect'
                case 'cooperate':
                    action, reason = 'choice_1', 'always cooperate'
                case _:
                    if self.log_enabled:
                        logger.error(""The nonexistent strategy was chosen."")
            if self.log_enabled:
                logger.debug(f""{self.name} chose action {action} with reason: {reason}"")
            return action, reason


# ========== Game Class Definition ==========
class Game:
    def __init__(self, agents, game_type, log_enabled=True):
        self.agents = agents
        self.game_type = game_type
        self.log_enabled = log_enabled

        if self.game_type == 'ovo':
            self._agent_a = self.agents[0]
            self._agent_b = self.agents[1]

        if self.log_enabled:
            logger.info(f""Initialized game of type {self.game_type} with agents: {', '.join([a.name for a in agents])}"")

    # ========== Logging Agent's Actions ==========
    def log_action(self, agent, roundn, action, opp_action, reason, reward):
        '''Logs the actions taken by the agent during a game round.

        This function constructs a log message that includes the round number,
        the agent's action, the opponent's action, the rewards received,
        and the reasoning behind the chosen action. If logging is enabled,
        this message is appended to the agent's previous messages and reasons.
        '''
        if self.log_enabled:
            log = f""Round #{roundn + 1}\nYour choice: {action}\nYour opponent's choice: {opp_action}"" \
                  f""\nReward: \n\tyours: {reward[0]} \n\topponent's: {reward[1]}\n"" \
                  f""My reason of picking this reward was: {reason}. "" \
                  f""Critique this reason or agree with it if it gives the best reward""
            agent.previous_message.append(log)
            agent.previous_reasons.append(reason)

    # ========== Play a Single Round (One-vs-One) ==========
    def play_single_round_ovo(self, roundn):
        alice_action, alice_reason = self._agent_a.make_action()
        bob_action, bob_reason = self._agent_b.make_action()
        reward = self._agent_a.payoffs[alice_action][bob_action]

        self.log_action(self._agent_a, roundn, alice_action, bob_action, alice_reason, reward)
        self.log_action(self._agent_b, roundn, bob_action, alice_action, bob_reason, list(reversed(reward)))

        if self.log_enabled:
            logger.info(f""Round {roundn + 1}: Alice chose {alice_action} and Bob chose {bob_action}"")
            logger.info(f""Rewards - Alice: {reward[0]}, Bob: {reward[1]}"")

        return (alice_action, alice_reason), (bob_action, bob_reason), reward

    # ========== Play the Complete Game (One-vs-One) ==========
    def play_ovo(self, n):
        results = {'Alice': 0, 'Bob': 0}
        for i in range(n):
            # Simulate round using function to play single round
            _, _, reward = self.",method_call,play_single_round_ovo(i),"
            results['Alice'] += reward[0]
            results['Bob'] += reward[1]

        if self.log_enabled:
            logger.info(f""Final results: {results}"")

        return results

",multi_agent_simulation.txt
13,"import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt


def extract_base_function(true_function):
    return true_function.split('.')[-1]


def ",function_name,check_function_in_generated,"(base_function, generated_code, log=False):
    try:
        possible_functions = re.findall(r'\b\w+\b', generated_code)
        results = base_function in possible_functions
        if log:
            print(f'True: \033[96m{base_function}\033[00m. \nGenerated: \033[96m{generated_code.strip()}\033[00m. '
                  f'\nResults: \033[96m{results}\033[00m\n')
        return results
    except:
        return False


# Define the models and read their datasets
num_samples = 3000
dfs = {
    'CodeStral-22B': pd.read_csv('../../data/generation/generation_results_codelstral_22B.csv')[:num_samples],
    'Mistral-7B': pd.read_csv('../../data/generation/generation_results_mistral_7B.csv')[:num_samples],
    'Phi-2': pd.read_csv('../../data/generation/generation_results_phi-2.csv')[:num_samples],
    'CodeGen2-3_7B': pd.read_csv('../../data/generation/generation_results_codegen2-3_7B.csv')[:num_samples],
    'CodeGen25-7B-Mono': pd.read_csv('../../data/generation/generation_results_codegen25-7b-monoB.csv')[:num_samples],
}

important_sections = ['DataFrame', 'General functions', 'GroupBy', 'Index objects', 'Input/output', 'Series']

overall_ratios = []
section_ratios_list = []

# Iterate through each model and process the data
for name, df in dfs.items():
    df = df[df['section_name'].isin(important_sections)]

    df['match_found'] = df.apply(
        lambda row: check_function_in_generated(extract_base_function(row['function_name']), row['generation']), axis=1)
    df_false = df[df.match_found == False]
    df_false = df_false[['function_name', 'description', 'section_name', 'subsection_name']]
    df_false.to_csv(f'{name}_false_gen.csv')
    print(len(df_false))
    overall_ratio = df.match_found.sum() / len(df)
    overall_ratios.append({'Model': name, 'Overall Correctness Ratio': overall_ratio})

    # Calculate the correctness ratio for each section
    section_ratios = df.groupby('section_name')['match_found'].mean().reset_index()
    section_ratios = section_ratios.rename(columns={'match_found': 'correctness_ratio'})
    section_counts = df.groupby('section_name').size().reset_index(name='row_count')
    section_ratios = pd.merge(section_ratios, section_counts, on='section_name')

    print(section_ratios.correctness_ratio.var())

    print(f'\nResults for {name} model:\nOverall correctness ratio: {overall_ratio:.4f}\n{section_ratios}')

    section_ratios['Model'] = name
    section_ratios_list.append(section_ratios)

# Combine section ratios for all models into a single DataFrame
section_ratios_df = pd.concat(section_ratios_list, ignore_index=True)
overall_ratios_df = pd.DataFrame(overall_ratios)

# Set up plotting style
sns.set(style=""whitegrid"")

# Plot 1: Overall Correctness Ratio for Each Model
plt.figure(figsize=(12, 8))
sns.barplot(x='Model', y='Overall Correctness Ratio', data=overall_ratios_df, palette=""muted"")
plt.title('Overall Correctness Ratio by Model')
plt.ylabel('Correctness Ratio')
plt.xlabel('Model')
plt.ylim(0, 1)
plt.savefig('../../data/plots/correctness_models.png', dpi=300)

# Plot 2: Correctness Ratio by Section for Each Model
plt.figure(figsize=(16, 9))
sns.barplot(x='correctness_ratio', y='section_name', hue='Model', data=section_ratios_df, palette=""muted"")
plt.title('Correctness Ratio by Section and Model')
plt.xlabel('Correctness Ratio')
plt.ylabel('Section Name')
plt.xlim(0, 1)
plt.legend(title='Model')
plt.savefig('../../data/plots/correctness_models_by_section.png', dpi=300)

plt.show()

",generation_eda.txt
14,"import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt


def extract_base_function(true_function):
    return true_function.split('.')[-1]


def check_function_in_generated(base_function, generated_code, ",function_parameter,log=False,"):
    try:
        possible_functions = re.findall(r'\b\w+\b', generated_code)
        results = base_function in possible_functions
        if log:
            print(f'True: \033[96m{base_function}\033[00m. \nGenerated: \033[96m{generated_code.strip()}\033[00m. '
                  f'\nResults: \033[96m{results}\033[00m\n')
        return results
    except:
        return False


# Define the models and read their datasets
num_samples = 3000
dfs = {
    'CodeStral-22B': pd.read_csv('../../data/generation/generation_results_codelstral_22B.csv')[:num_samples],
    'Mistral-7B': pd.read_csv('../../data/generation/generation_results_mistral_7B.csv')[:num_samples],
    'Phi-2': pd.read_csv('../../data/generation/generation_results_phi-2.csv')[:num_samples],
    'CodeGen2-3_7B': pd.read_csv('../../data/generation/generation_results_codegen2-3_7B.csv')[:num_samples],
    'CodeGen25-7B-Mono': pd.read_csv('../../data/generation/generation_results_codegen25-7b-monoB.csv')[:num_samples],
}

important_sections = ['DataFrame', 'General functions', 'GroupBy', 'Index objects', 'Input/output', 'Series']

overall_ratios = []
section_ratios_list = []

# Iterate through each model and process the data
for name, df in dfs.items():
    df = df[df['section_name'].isin(important_sections)]

    df['match_found'] = df.apply(
        lambda row: check_function_in_generated(extract_base_function(row['function_name']), row['generation']), axis=1)
    df_false = df[df.match_found == False]
    df_false = df_false[['function_name', 'description', 'section_name', 'subsection_name']]
    df_false.to_csv(f'{name}_false_gen.csv')
    print(len(df_false))
    overall_ratio = df.match_found.sum() / len(df)
    overall_ratios.append({'Model': name, 'Overall Correctness Ratio': overall_ratio})

    # Calculate the correctness ratio for each section
    section_ratios = df.groupby('section_name')['match_found'].mean().reset_index()
    section_ratios = section_ratios.rename(columns={'match_found': 'correctness_ratio'})
    section_counts = df.groupby('section_name').size().reset_index(name='row_count')
    section_ratios = pd.merge(section_ratios, section_counts, on='section_name')

    print(section_ratios.correctness_ratio.var())

    print(f'\nResults for {name} model:\nOverall correctness ratio: {overall_ratio:.4f}\n{section_ratios}')

    section_ratios['Model'] = name
    section_ratios_list.append(section_ratios)

# Combine section ratios for all models into a single DataFrame
section_ratios_df = pd.concat(section_ratios_list, ignore_index=True)
overall_ratios_df = pd.DataFrame(overall_ratios)

# Set up plotting style
sns.set(style=""whitegrid"")

# Plot 1: Overall Correctness Ratio for Each Model
plt.figure(figsize=(12, 8))
sns.barplot(x='Model', y='Overall Correctness Ratio', data=overall_ratios_df, palette=""muted"")
plt.title('Overall Correctness Ratio by Model')
plt.ylabel('Correctness Ratio')
plt.xlabel('Model')
plt.ylim(0, 1)
plt.savefig('../../data/plots/correctness_models.png', dpi=300)

# Plot 2: Correctness Ratio by Section for Each Model
plt.figure(figsize=(16, 9))
sns.barplot(x='correctness_ratio', y='section_name', hue='Model', data=section_ratios_df, palette=""muted"")
plt.title('Correctness Ratio by Section and Model')
plt.xlabel('Correctness Ratio')
plt.ylabel('Section Name')
plt.xlim(0, 1)
plt.legend(title='Model')
plt.savefig('../../data/plots/correctness_models_by_section.png', dpi=300)

plt.show()

",generation_eda.txt
15,"import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt


def extract_base_function(true_function):
    return true_function.split('.')[-1]


def check_function_in_generated(base_function, generated_code, log=False):
    try:
        possible_functions = re.findall(r'\b\w+\b', generated_code)
        results = base_function in possible_functions
        if ",conditional_statement,log,":
            print(f'True: \033[96m{base_function}\033[00m. \nGenerated: \033[96m{generated_code.strip()}\033[00m. '
                  f'\nResults: \033[96m{results}\033[00m\n')
        return results
    except:
        return False


# Define the models and read their datasets
num_samples = 3000
dfs = {
    'CodeStral-22B': pd.read_csv('../../data/generation/generation_results_codelstral_22B.csv')[:num_samples],
    'Mistral-7B': pd.read_csv('../../data/generation/generation_results_mistral_7B.csv')[:num_samples],
    'Phi-2': pd.read_csv('../../data/generation/generation_results_phi-2.csv')[:num_samples],
    'CodeGen2-3_7B': pd.read_csv('../../data/generation/generation_results_codegen2-3_7B.csv')[:num_samples],
    'CodeGen25-7B-Mono': pd.read_csv('../../data/generation/generation_results_codegen25-7b-monoB.csv')[:num_samples],
}

important_sections = ['DataFrame', 'General functions', 'GroupBy', 'Index objects', 'Input/output', 'Series']

overall_ratios = []
section_ratios_list = []

# Iterate through each model and process the data
for name, df in dfs.items():
    df = df[df['section_name'].isin(important_sections)]

    df['match_found'] = df.apply(
        lambda row: check_function_in_generated(extract_base_function(row['function_name']), row['generation']), axis=1)
    df_false = df[df.match_found == False]
    df_false = df_false[['function_name', 'description', 'section_name', 'subsection_name']]
    df_false.to_csv(f'{name}_false_gen.csv')
    print(len(df_false))
    overall_ratio = df.match_found.sum() / len(df)
    overall_ratios.append({'Model': name, 'Overall Correctness Ratio': overall_ratio})

    # Calculate the correctness ratio for each section
    section_ratios = df.groupby('section_name')['match_found'].mean().reset_index()
    section_ratios = section_ratios.rename(columns={'match_found': 'correctness_ratio'})
    section_counts = df.groupby('section_name').size().reset_index(name='row_count')
    section_ratios = pd.merge(section_ratios, section_counts, on='section_name')

    print(section_ratios.correctness_ratio.var())

    print(f'\nResults for {name} model:\nOverall correctness ratio: {overall_ratio:.4f}\n{section_ratios}')

    section_ratios['Model'] = name
    section_ratios_list.append(section_ratios)

# Combine section ratios for all models into a single DataFrame
section_ratios_df = pd.concat(section_ratios_list, ignore_index=True)
overall_ratios_df = pd.DataFrame(overall_ratios)

# Set up plotting style
sns.set(style=""whitegrid"")

# Plot 1: Overall Correctness Ratio for Each Model
plt.figure(figsize=(12, 8))
sns.barplot(x='Model', y='Overall Correctness Ratio', data=overall_ratios_df, palette=""muted"")
plt.title('Overall Correctness Ratio by Model')
plt.ylabel('Correctness Ratio')
plt.xlabel('Model')
plt.ylim(0, 1)
plt.savefig('../../data/plots/correctness_models.png', dpi=300)

# Plot 2: Correctness Ratio by Section for Each Model
plt.figure(figsize=(16, 9))
sns.barplot(x='correctness_ratio', y='section_name', hue='Model', data=section_ratios_df, palette=""muted"")
plt.title('Correctness Ratio by Section and Model')
plt.xlabel('Correctness Ratio')
plt.ylabel('Section Name')
plt.xlim(0, 1)
plt.legend(title='Model')
plt.savefig('../../data/plots/correctness_models_by_section.png', dpi=300)

plt.show()

",generation_eda.txt
16,from transformers import ,imports,"AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig","
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:
        code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt
17,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = ",var_declaration,datetime.datetime.now(),"

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:
        code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt
18,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = ",var_declaration,"BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)","

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:
        code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt
19,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.",method_call,"from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)","

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:
        code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt
20,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""",description_by_code,"    Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:
        code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.","
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt
21,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:
        code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = ",code_by_description,"[]
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data","


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt
22,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:
        code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = ",method_call,read_jsonl_file(file_path),"
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt
23,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:
        code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.",method_call,read_csv(file_path),"
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt
24,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:
        code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(",method_call,"sample[""text""], padding=True, truncation=True, max_length=256",")
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt
25,"from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
import datetime
import torch
import random
import numpy as np
import json
import re
import pandas as pd
from tqdm import tqdm
import Levenshtein
import nltk
from rouge_score import rouge_scorer
import tree_sitter
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset
import os

# ========== Model Loading ==========
# Set device to CUDA (GPU), load a pre-trained tokenizer and model (phi-1_5) using 4-bit quantization for efficiency.
print(""Loading model..."")
time = datetime.datetime.now()

tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-1_5"")
tokenizer.pad_token = tokenizer.eos_token

# Load common 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    ""microsoft/phi-1_5"",
    quantization_config=bnb_config,
    trust_remote_code=True,
)

time1 = datetime.datetime.now()
print(f""Model loaded. Time to load the model: {time1 - time}"")

# ========== LoRA Configuration ==========
# Setup LoRA (Low-Rank Adaptation) for efficient fine-tuning, specifically targeting certain transformer layers.
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=[""dense"", ""fc2"", ""q_proj"", ""k_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# ========== Tag Replacement Function ==========
def replace_tags(code):
    """"""Replaces special tags in the input code with their corresponding literals or empty strings.

    Parameters:
        code (str): The input code containing special tags.

    Returns:
        str: The code with tags replaced by literals or empty strings.
    """"""
    code = code.replace("""", ""0"").replace("""", """").replace("""", """")
    pattern = re.compile(r""<(STR|NUM|CHAR)_LIT:(.*?)>"", re.S)
    lits = re.findall(pattern, code)
    for lit in lits:
        code = code.replace(f""<{lit[0]}_LIT:{lit[1]}>"", lit[1])
    pattern = r'<([A-Z][^<>]*)>'
    liners = re.findall(pattern, code)
    for tag in liners:
        code = code.replace(f'<{tag}>', ' ')
    return code


# ========== JSONL File Reader ==========
def read_jsonl_file(file_path):
    """"""
    Reads a JSONL file and replaces special tags in the 'signature' and 'body' fields of each JSON object.

    Parameters:
        file_path (str): The path to the JSONL file.

    Returns:
        list: A list of dictionaries, each containing the modified JSON objects.
        Each object contains 'signature' and 'body', obtained by applying replace_tags function.
    """"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            json_obj = json.loads(line)
            json_obj['signature'] = replace_tags(json_obj['signature'])
            json_obj['body'] = replace_tags(json_obj['body'])
            data.append(json_obj)
    return data


file_path = '/content/drive/MyDrive/CodeCompletion/CodeXGlue/test.jsonl'
codexglue_test = read_jsonl_file(file_path)
print(f'{codexglue_test[0]}\n')


# ========== Data Loading and Preprocessing ==========
# Load and convert function datasets into the proper format for tokenization and training.
columns_to_convert = ['is_single_expression', 'is_test', '0-20', '100+', '20-50', '50-100']

file_path = '/content/drive/MyDrive/CodeCompletion/functions_df_inputs_outputs.csv'
functions_df = pd.read_csv(file_path)
functions_df[columns_to_convert] = functions_df[columns_to_convert].astype(str)
print(f'{functions_df.iloc[0]}\n')

file_path = '/content/drive/MyDrive/CodeCompletion/context_functions_df.csv'
context_functions_df = pd.read_csv(file_path)
context_functions_df[columns_to_convert] = context_functions_df[columns_to_convert].astype(str)
print(f'{context_functions_df.iloc[0]}\n')


# ========== Tokenization ==========
# Tokenizes the dataset by combining function signature and body, preparing it for training.
def tokenize(sample):
    tokenized_text = tokenizer(sample[""text""], padding=True, truncation=True, max_length=256)
    return tokenized_text


functions_df[""text""] = functions_df[[""signature"", ""body""]].apply(
    lambda x: ""Prompt: "" + x[""signature""] + "" Completion: "" + x[""body""], axis=1)
print(functions_df.iloc[0])

data = Dataset.from_pandas(functions_df)
tokenized_data = data.map(tokenize, batched=True, desc=""Tokenizing data"", remove_columns=data.column_names)

# ========== Training Setup and Execution ==========
# Define the training arguments such as batch size, learning rate, and number of epochs for fine-tuning the model.
training_arguments = TrainingArguments(
    output_dir=""phi-1_5-finetuned-kotlin"",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=2e-4,
    lr_scheduler_type=""cosine"",
    save_strategy=""epoch"",
    logging_steps=100,
    max_steps=1000,
    num_train_epochs=1
)

trainer = ",var_declaration,"Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)","

trainer.train()

# ========== Saving and Merging the Model ==========
# Save the fine-tuned model and load it again for inference.
model.save_pretrained(""phi-1_5-finetuned-kotlin"")

model = AutoModelForCausalLM.from_pretrained(""microsoft/phi-1_5"", trust_remote_code=True, torch_dtype=torch.float32)
peft_model = PeftModel.from_pretrained(model, ""phi-1_5-finetuned-kotlin"", from_transformers=True)
model = peft_model.merge_and_unload()

",initialize_lora_model.txt
26,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import ",imports,"Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, \
    GlobalAveragePooling2D, concatenate","
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt
27,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, \
    GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            ",class_initialization,"self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])","

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt
28,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, \
    GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    ",code_by_description,"x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)","
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt
29,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, \
    GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(",function_parameter,"input_shape, num_classes",", growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt
30,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, \
    GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = ",var_declaration,"transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])","


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt
31,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, \
    GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, ",function_parameter,batch_size,"):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt
32,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, \
    GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        ",class_initialization,"self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf","

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt
33,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, \
    GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = ",var_declaration,"create_densenet((32, 32, 3), 10)","
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt
34,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, \
    GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.",method_call,"TensorBoard(log_dir=logdir, histogram_freq=1)","
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt
35,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, \
    GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = ",var_declaration,"int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))","
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.fit(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt
36,"import datetime
import numpy as np
import os
import sys
import urllib.request
import re
import keras
import torch

from keras.models import Model
# Import all common layer types from keras + layers for 2D-transformation
from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Conv2D, MaxPooling2D, AveragePooling2D, \
    GlobalAveragePooling2D, concatenate
from torchvision import transforms
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard


# ========== Dataset Definition ==========
class CIFAR10:
    H, W, C = 32, 32, 3
    LABELS = [""airplane"", ""automobile"", ""bird"", ""cat"", ""deer"", ""dog"", ""frog"", ""horse"", ""ship"", ""truck""]
    _URL = ""https://ufal.mff.cuni.cz/some_dataset.npz""

    class Dataset:
        def __init__(self, data, seed=42):
            self._data = data
            self._data[""labels""] = self._data[""labels""].ravel()
            self._size = len(self._data[""images""])

        @property
        def data(self):
            return self._data

        @property
        def size(self):
            return self._size

        def dataset(self, transform=None):
            class TorchDataset(torch.utils.data.Dataset):
                def __len__(self):
                    return self._size

                def __getitem__(self, index):
                    item = {key: value[index] for key, value in self._data.items()}
                    if transform is not None:
                        item = transform(item)
                    return item

            return TorchDataset()

    def __init__(self, size={}):
        path = os.path.basename(self._URL)
        if not os.path.exists(path):
            print(""Downloading CIFAR-10 dataset..."", file=sys.stderr)
            urllib.request.urlretrieve(self._URL, filename=f""{path}.tmp"")
            os.rename(f""{path}.tmp"", path)

        cifar = np.load(path)
        for dataset in [""train"", ""dev"", ""test""]:
            data = {key[len(dataset) + 1:]: cifar[key][:size.get(dataset, None)] for key in cifar if
                    key.startswith(dataset)}
            setattr(self, dataset, self.Dataset(data))


# ========== Model Layers Definition ==========
def bn_relu_conv(x, filters, kernel_size=3, strides=1, padding='same'):
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(filters, kernel_size, strides=strides, padding=padding, kernel_initializer='he_normal')(x)
    return x


def bottleneck_layer(x, growth_rate):
    x = bn_relu_conv(x, 4 * growth_rate, kernel_size=1)
    x = bn_relu_conv(x, growth_rate, kernel_size=3)
    return x


def transition_layer(x, compression_factor=0.5):
    reduced_filters = int(x.shape[-1] * compression_factor)
    x = bn_relu_conv(x, reduced_filters, kernel_size=1)
    x = AveragePooling2D(pool_size=(2, 2), strides=2)(x)
    return x


def dense_block(x, num_layers, growth_rate):
    for i in range(num_layers):
        layer = bottleneck_layer(x, growth_rate)
        x = concatenate([x, layer])
    return x


# ========== Model Creation ==========
def create_densenet(input_shape, num_classes, growth_rate=24, block_layers=[6, 12, 24, 16], compression_factor=0.5):
    inputs = Input(shape=input_shape)
    x = bn_relu_conv(inputs, growth_rate * 2, kernel_size=3)

    for i, layers in enumerate(block_layers):
        x = dense_block(x, num_layers=layers, growth_rate=growth_rate)
        if i != len(block_layers) - 1:
            x = transition_layer(x, compression_factor=compression_factor)

    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = GlobalAveragePooling2D()(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model


# ========== Data Augmentation ==========
# Image transformations for data augmentation:
# - Random flip, rotate by 15 degrees, crop resize to size 32 for varied inputs.
padding = 4
scale, ratio = (0.8, 1.0), (0.99, 1.01)
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomCrop(32, padding=padding),
    transforms.RandomResizedCrop(32, scale=scale, ratio=ratio)
])


def pytorch_augment_data_generator(X, y, batch_size):
    while True:
        indices = np.arange(len(X))
        np.random.shuffle(indices)
        for start in range(0, len(X), batch_size):
            end = start + batch_size
            batch_indices = indices[start:end]
            batch_images = X[batch_indices]
            batch_labels = y[batch_indices]

            augmented_images = []
            for image in batch_images:
                image = np.transpose(image, (2, 0, 1))
                image_tensor = torch.tensor(image, dtype=torch.float)
                image_transformed = transform(image_tensor).numpy()
                image_transformed = np.transpose(image_transformed, (1, 2, 0))
                augmented_images.append(image_transformed)

            yield np.array(augmented_images), np.array(batch_labels)


# ========== Training Settings ==========
class Args:
    batch_size = 64
    epochs = 250
    seed = 42
    threads = 8


args = Args()


# ========== Custom Callback for Model Checkpointing ==========
class CustomModelCheckpoint(keras.callbacks.Callback):
    def __init__(self, filepath, monitor='val_accuracy', mode='max', threshold=0.935):
        super(CustomModelCheckpoint, self).__init__()
        self.filepath = filepath
        self.monitor = monitor
        self.mode = mode
        self.threshold = threshold
        self.best = 0 if mode == 'max' else np.inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if self.mode == 'max' and current > self.best and current > self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)
        elif self.mode == 'min' and current < self.best and current < self.threshold:
            self.best = current
            self.model.save(self.filepath.format(epoch=epoch + 1, val_accuracy=current), overwrite=True)


# ========== Model Compilation and Callbacks Setup ==========
logdir = os.path.join(""logs"", ""{}-{}-{}"".format(
    os.path.basename(globals().get(""__file__"", ""notebook"")),
    datetime.datetime.now().strftime(""%Y-%m-%d_%H%M%S""),
    "","".join((""{}={}"".format(re.sub(""(.)[^_]*_?"", r""\1"", k), v) for k, v in sorted(vars(args).items())))
))

cifar = CIFAR10()

# Create DenseNet using function above
model = create_densenet((32, 32, 3), 10)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1), metrics=['accuracy'])

tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)
early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1,
                                                        restore_best_weights=True)
cifar.train.data[""labels""] = keras.utils.to_categorical(cifar.train.data[""labels""], num_classes=10)
cifar.dev.data[""labels""] = keras.utils.to_categorical(cifar.dev.data[""labels""], num_classes=10)

steps_per_epoch = int(np.ceil(len(cifar.train.data[""images""]) / args.batch_size))
lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=6, min_lr=0.1e-6)

checkpoint_filepath = '/content/drive/MyDrive/cifar/model_checkpoint4-{val_accuracy:.4f}.keras'

model_checkpoint_callback = CustomModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    threshold=0.935
)

# ========== Model Training ==========
model.",method_call,fit,"(
    pytorch_augment_data_generator(cifar.train.data[""images""], cifar.train.data[""labels""], args.batch_size),
    steps_per_epoch=steps_per_epoch,
    epochs=args.epochs,
    validation_data=(cifar.dev.data[""images""], cifar.dev.data[""labels""]),
    callbacks=[tensorboard_callback, lr_reducer, model_checkpoint_callback]
)

",cifar_classification.txt
