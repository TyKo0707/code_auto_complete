{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Info\n",
        "If you didn't see `explore_tiny_starcoder.ipynb` notebook yet, better make it before looking into this notebook since it is a continuation of experiments from there.\n"
      ],
      "metadata": {
        "id": "amupnYW20sK1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "X6MoVkZkopr2"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approximate VRAM usage is 3.5GB"
      ],
      "metadata": {
        "id": "r-fd4--YrGNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model_name = \"bigcode/starcoder2-3b\"\n",
        "device = \"cuda\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config).to(device)"
      ],
      "metadata": {
        "id": "MbjX_Xu9ptRI"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About Model\n",
        "\n",
        "**Starcoder3B** is built on the **Starcoder2Model** architecture, designed for code generation.\n",
        "\n",
        "It consists of:\n",
        "\n",
        "- Embedding layer (**embed_tokens**):  Maps input tokens to a 3072-dimensional space with a vocabulary of 49,152 tokens.\n",
        "\n",
        "- 30 Transformer blocks (**Starcoder2DecoderLayer**):\n",
        "  Each block includes:\n",
        "  - Self-attention (**self_attn**):\n",
        "    - Linear layers for queries (**q_proj**), keys (**k_proj**), values (**v_proj**), and output (**o_proj**) with inputs/outputs of 3072 and 256 dimensions.\n",
        "    - Rotary embedding (**rotary_emb**) for positional encoding.\n",
        "  - Feed-forward network (**mlp**):\n",
        "    - Expands dimensions (c_fc: 3072 to 12288) and projects back (**c_proj**: 12288 to 3072).\n",
        "    - Activation function: Uses **PytorchGELUTanh**.\n",
        "  - Layer normalization: Applied before and after attention.\n",
        "\n",
        "- Final layer normalization (**norm**): Stabilizes training.\n",
        "\n",
        "- Language modeling head (**lm_head**):\n",
        "  A linear layer mapping the 3072-dimensional hidden state to 49,152 tokens for output generation.<br><br>\n",
        "\n",
        "**Interesting note on the differences in embeddings between two models** (Starcoder3B and tiny_starcoder_py):\n",
        "\n",
        "Rotary embedding layers use a rotary mechanism to encode positional information, capturing relative positions and improving attention across varying input lengths. This enhances generalization for different sequence lengths.\n",
        "\n",
        "In contrast, traditional word position embeddings assign fixed vectors to absolute positions, which are less flexible and may not generalize well to longer sequences."
      ],
      "metadata": {
        "id": "EykA9Ve-yOmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzwqXK5NvLNK",
        "outputId": "a9a68875-bf0f-4dfb-9dd5-7291075e6cf2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Starcoder2ForCausalLM(\n",
              "  (model): Starcoder2Model(\n",
              "    (embed_tokens): Embedding(49152, 3072)\n",
              "    (layers): ModuleList(\n",
              "      (0-29): 30 x Starcoder2DecoderLayer(\n",
              "        (self_attn): Starcoder2SdpaAttention(\n",
              "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=True)\n",
              "          (k_proj): Linear8bitLt(in_features=3072, out_features=256, bias=True)\n",
              "          (v_proj): Linear8bitLt(in_features=3072, out_features=256, bias=True)\n",
              "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=True)\n",
              "          (rotary_emb): Starcoder2RotaryEmbedding()\n",
              "        )\n",
              "        (mlp): Starcoder2MLP(\n",
              "          (c_fc): Linear8bitLt(in_features=3072, out_features=12288, bias=True)\n",
              "          (c_proj): Linear8bitLt(in_features=12288, out_features=3072, bias=True)\n",
              "          (act): PytorchGELUTanh()\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=49152, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Model\n",
        "The generation of the middle part for this model is slightly different from the tiny one. You can see a full thread in this [issue](https://github.com/bigcode-project/starcoder2/issues/10), I try to summarize:\n",
        "- Suffix and prefix settings are performed using the same tokens as for tiny_starcoder_py: `<fim_prefix>`, `<fim_suffix>` and `<fim_middle>`.\n",
        "- Model does not always generate `<|endoftext|>` to end generation, sometimes it happens to get `<file_sep>` instead. <br><br>\n",
        "\n",
        "Examples are quite similar but some of them are a little complicated since I want to see actual model capabilities.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t9M6sB0J1ZOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_middle_output(text):\n",
        "    prefix = re.search('<fim_prefix>(.*?)<fim_suffix>', text, re.DOTALL).group(1)\n",
        "    suffix = re.search('<fim_suffix>(.*?)<fim_middle>', text, re.DOTALL).group(1)\n",
        "    try:\n",
        "        output = re.search('<fim_middle>(.*?)<file_sep>', text, re.DOTALL).group(1)\n",
        "    except:\n",
        "        output = re.search('<fim_middle>(.*)', text).group(1).replace('<|endoftext|>', '')\n",
        "    return prefix + output + suffix"
      ],
      "metadata": {
        "id": "_OvB5rbItEyo"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(7):\n",
        "  print(tokenizer.decode(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwazprFV2pmq",
        "outputId": "514d0f13-ac11-4794-b26b-ce166035840c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|endoftext|>\n",
            "<fim_prefix>\n",
            "<fim_middle>\n",
            "<fim_suffix>\n",
            "<fim_pad>\n",
            "<repo_name>\n",
            "<file_sep>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing configuration parameters\n",
        "In detail, parameters are described in the notebook for tiny_starcoder_py. I will use the same ones here since it seems to work well.\n"
      ],
      "metadata": {
        "id": "Q3qMpLiq3nTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'max_new_tokens': 128,\n",
        "    'temperature': 0.2,\n",
        "    'top_k': 50,\n",
        "    'top_p': 0.1,\n",
        "    'repetition_penalty': 1.17,\n",
        "    'do_sample': True\n",
        "}"
      ],
      "metadata": {
        "id": "zc61tzE_sE0J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 1\n",
        "Initialize model using `from_pretrained`."
      ],
      "metadata": {
        "id": "YoKXCx6uhaqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix_load_model = \"<fim_prefix>base_model_id = 'microsoft/phi-2'\\nmodel = \"\n",
        "suffix_load_model = \"<fim_suffix>tokenizer = AutoTokenizer.from_pretrained(model_name)\\n\"\n",
        "input_text = prefix_load_model + suffix_load_model + '<fim_middle>'\n",
        "\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, **params)\n",
        "print(f'\\033[96m {format_middle_output(tokenizer.decode(outputs[0]))} \\033[00m')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gLevrSIvU7k",
        "outputId": "4583e49f-e3fa-4605-de2e-a536eaf980e7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m base_model_id ='microsoft/phi-2'\n",
            "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to('cuda')\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
            " \u001b[00m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 2\n",
        "Initialize model using `from_pretrained` with additional comment."
      ],
      "metadata": {
        "id": "lcpbZSTohezn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix_load_model_comment = \"<fim_prefix># Initialize model and set load_in_8bit to True\\nbase_model_id = 'microsoft/phi-2'\\nmodel = \"\n",
        "suffix_load_model_comment = \"<fim_suffix>tokenizer = AutoTokenizer.from_pretrained(model_name)\\n\"\n",
        "input_text = prefix_load_model_comment + suffix_load_model_comment + '<fim_middle>'\n",
        "\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, **params)\n",
        "print(f'\\033[96m {format_middle_output(tokenizer.decode(outputs[0]))} \\033[00m')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfXbcy8RvWPr",
        "outputId": "e2a4eeff-b40a-4900-b6c2-7aab7f5c2adf"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m # Initialize model and set load_in_8bit to True\n",
            "base_model_id ='microsoft/phi-2'\n",
            "model = AutoModelForCausalLM.from_pretrained(\n",
            "    base_model_id, load_in_8bit=True).to(\"cuda\")\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
            " \u001b[00m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 3\n",
        "Tokenize labels within tokenization function."
      ],
      "metadata": {
        "id": "ZpvJ5S2chfs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix_tokenize_labels = \"\"\"<fim_prefix>def tokenize(prompt): \\nresult = tokenizer(prompt['prompt'], max_length=max_input_length, truncation=True, padding=True)\\n\"\"\"\n",
        "suffix_tokenize_labels = \"\"\"<fim_suffix>\\nresult[\"labels\"] = labels[\"input_ids\"] \\n return result\"\"\"\n",
        "input_text = prefix_tokenize_labels + suffix_tokenize_labels + '<fim_middle>'\n",
        "\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, **params)\n",
        "print(f'\\033[96m {format_middle_output(tokenizer.decode(outputs[0]))} \\033[00m')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2IUsMNXvXiw",
        "outputId": "5475c972-daee-4da6-d7f5-66b1b1635ab8"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m def tokenize(prompt): \n",
            "result = tokenizer(prompt['prompt'], max_length=max_input_length, truncation=True, padding=True)\n",
            "result.update({\"input_ids\": prompt[0], \"attention_mask\" : prompt[1]})\n",
            "result[\"labels\"] = labels[\"input_ids\"] \n",
            " return result \u001b[00m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 4\n",
        "Map tokenization function to the dataset."
      ],
      "metadata": {
        "id": "B8F2kaVhhgfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix_map_tokenization = \"\"\"<fim_prefix>def generate_and_tokenize_prompt(data_point):\\n\\treturn tokenize(data_point)\\ntokenized_train_dataset = \"\"\"\n",
        "suffix_map_tokenization = \"<fim_suffix>tokenized_train_dataset = tokenized_train_dataset.remove_columns(['prompt', 'function_name'])\"\n",
        "input_text = prefix_map_tokenization + suffix_map_tokenization + '<fim_middle>'\n",
        "\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, **params)\n",
        "print(f'\\033[96m {format_middle_output(tokenizer.decode(outputs[0]))} \\033[00m')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4KZi7t2rvRy",
        "outputId": "54602d89-b138-4e24-ee14-72cd7e6bbfca"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m def generate_and_tokenize_prompt(data_point):\n",
            "\treturn tokenize(data_point)\n",
            "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt, batched=True).filter(lambda x: len(x['input_ids'][0]) > 128 and len(x['input_ids'][0]) < 513 )\n",
            "\n",
            "tokenized_train_dataset = tokenized_train_dataset.remove_columns(['prompt', 'function_name']) \u001b[00m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Results Analysis\n",
        "The model performed well with the provided examples, generating coherent code, but there were some areas where improvements could be made:\n",
        "- Example 1: The model correctly initializes the model, everything is alright.\n",
        "- Example 2: Same as Ex.1\n",
        "- Example 3: Models does not understand that there is a variable of labels missing and that it needs to generate it.\n",
        "- Example 4: The model effectively mapped the function to the dataset and added filtering, but the filtering condition's complexity may need clarification regarding its purpose."
      ],
      "metadata": {
        "id": "-OlwUpQT5nR8"
      }
    }
  ]
}