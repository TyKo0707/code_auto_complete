{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KSzC85L-DUfh"
      },
      "outputs": [],
      "source": [
        "# pip install -q transformers==4.29.2\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"bigcode/tiny_starcoder_py\"\n",
        "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju-M-x5DbGkA",
        "outputId": "957aca3e-f9e4-4759-81f7-99b07f6a79d4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About Model\n",
        "**tiny_starcoder_py** built using the **GPTBigCodeForCausalLM** architecture, which is a transformer-based model designed for code generation.\n",
        "\n",
        "It consists of:\n",
        "- Embedding layers: Word token embedding (**wte**) and positional embedding (**wpe**), both with 768-dimensional outputs.\n",
        "- 20 Transformer blocks (**GPTBigCodeBlock**): Each block has multi-head attention with dropout set to 0.1, layer normalization, and a feed-forward network (expanding to 3072 dimensions + GELU activation).\n",
        "- Final layer norm (**ln_f**): Applied after the transformer layers.\n",
        "- Language modeling head (**lm_head**): Maps the 768-dimensional hidden state back to the vocabulary of 49,152 tokens."
      ],
      "metadata": {
        "id": "YKIm6dDZkzfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUGwXVFovvoo",
        "outputId": "cbc403a7-0f73-4902-c809-0f8bda20b076"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTBigCodeForCausalLM(\n",
              "  (transformer): GPTBigCodeModel(\n",
              "    (wte): Embedding(49152, 768)\n",
              "    (wpe): Embedding(8192, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-19): 20 x GPTBigCodeBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTBigCodeSdpaAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=896, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPTBigCodeMLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): PytorchGELUTanh()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=49152, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Model\n",
        "Taking examples from the creator of the model ([repo](https://github.com/the-crypt-keeper/tiny_starcoder/tree/main)), we can try to either replicate them and assure that model works in the right way or to run our own completions."
      ],
      "metadata": {
        "id": "K6kC-eaFmYLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing configuration parameters\n",
        "We can either use greedy decoding (which is basically selecting the most probable token in each step) or use some of the sampling methods (Beam search, Top-k sampling, Top-p sampling, or lots of others that not available on [HuggingFace](https://huggingface.co/docs/transformers/generation_strategies#customize-text-generation))\n",
        "\n",
        "They recommend to use following parameters for default generation tasks:\n",
        "- *max_new_tokens* (128): Limits the number of tokens generated to 128.\n",
        "- *temperature* (0.2): Controls randomness: lower values make the output more deterministic, while higher values increase creativity.\n",
        "- *top_k* (50): Restricts sampling to the top 50 most probable tokens at each step, limiting less likely options.\n",
        "- *top_p* (0.1): Samples from the smallest set of tokens whose cumulative probability reaches 10%, balancing diversity.\n",
        "- *repetition_penalty* (1.17): Penalizes repeated tokens to reduce loops and ensure variety in generated output.\n",
        "- *do_sample* (True): Enables sampling, introducing randomness into the generation instead of always choosing the most probable token.\n",
        "\n",
        "Will experiment with these parameters later."
      ],
      "metadata": {
        "id": "NIeG46EGoOmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sane hyper-parameters\n",
        "params = {\n",
        "    'max_new_tokens': 128,\n",
        "    'temperature': 0.2,\n",
        "    'top_k': 50,\n",
        "    'top_p': 0.1,\n",
        "    'repetition_penalty': 1.17,\n",
        "    'do_sample': True\n",
        "}"
      ],
      "metadata": {
        "id": "tXm54_NrDcAE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_middle_output(text):\n",
        "    prefix = re.search('<fim_prefix>(.*)<fim_suffix>', text, re.DOTALL).group(1)\n",
        "    suffix = re.search('<fim_suffix>(.*)<fim_middle>', text, re.DOTALL).group(1)\n",
        "    output = re.search('<fim_middle>(.*)', text).group(1).replace('<|endoftext|>', '')\n",
        "    return prefix + output + suffix"
      ],
      "metadata": {
        "id": "sv1Qr5FiavIy"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt Style 1: Function Signature\n",
        "inputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, **params)\n",
        "print(f'Prompt Style 1: Function Signature\\n\\033[96m {tokenizer.decode(outputs[0])} \\033[00m\\n\\n')\n",
        "\n",
        "# Prompt Style 2: A comment\n",
        "inputs = tokenizer.encode(\"# a python function that says hello\\n\", return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, **params)\n",
        "print(f'Prompt Style 2: A comment\\n\\033[96m {tokenizer.decode(outputs[0])} \\033[00m\\n\\n')\n",
        "\n",
        "# Prompt Style 3: A docstring\n",
        "inputs = tokenizer.encode(\"\\\"\\\"\\\" a python function that says hello \\\"\\\"\\\"\\n\", return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, **params)\n",
        "print(f'Prompt Style 3: A docstring\\n\\033[96m {tokenizer.decode(outputs[0])} \\033[00m\\n\\n')\n",
        "\n",
        "# Prompt Style 4: [ADVANCED] Fill in the middle\n",
        "input_text = \"<fim_prefix>def print_one_two_three():\\n    print('one')\\n    <fim_suffix>\\n    print('three')<fim_middle>\"\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, **params)\n",
        "print(f'Prompt Style 4: [ADVANCED] Fill in the middle (w/o processing)\\n\\033[96m {tokenizer.decode(outputs[0])}\\n \\033[00m')\n",
        "print(f'Prompt Style 4: [ADVANCED] Fill in the middle (Processing)\\n\\033[96m {format_middle_output(tokenizer.decode(outputs[0]))} \\033[00m')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9omgAUTDtyY",
        "outputId": "0ab9a34a-00e7-412e-ba51-18bd0b69d73a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt Style 1: Function Signature\n",
            "\u001b[96m def print_hello_world():\n",
            "    \"\"\"Prints hello world\"\"\"\n",
            "\n",
            "    print(\"Hello World!\")\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "<|endoftext|> \u001b[00m\n",
            "\n",
            "\n",
            "Prompt Style 2: A comment\n",
            "\u001b[96m # a python function that says hello\n",
            "def say_hello():\n",
            "    print(\"Hello World!\")\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    say_hello()<|endoftext|> \u001b[00m\n",
            "\n",
            "\n",
            "Prompt Style 3: A docstring\n",
            "\u001b[96m \"\"\" a python function that says hello \"\"\"\n",
            "def say_hello():\n",
            "    print(\"Hello World!\")\n",
            "\n",
            "<|endoftext|> \u001b[00m\n",
            "\n",
            "\n",
            "Prompt Style 4: [ADVANCED] Fill in the middle (w/o processing)\n",
            "\u001b[96m <fim_prefix>def print_one_two_three():\n",
            "    print('one')\n",
            "    <fim_suffix>\n",
            "    print('three')<fim_middle>print('two')<|endoftext|>\n",
            " \u001b[00m\n",
            "Prompt Style 4: [ADVANCED] Fill in the middle (Processing)\n",
            "\u001b[96m def print_one_two_three():\n",
            "    print('one')\n",
            "    print('two')\n",
            "    print('three') \u001b[00m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Own examples\n",
        "As we can see, the model does fairly well on the example jobs. Let's attempt our own examples. Since our task is to generate code using prefix and suffix, I will use Prompt Style 4."
      ],
      "metadata": {
        "id": "GBvvfXFlZc3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 1\n",
        "Initialize model using `from_pretrained`."
      ],
      "metadata": {
        "id": "YoKXCx6uhaqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix_load_model = \"<fim_prefix>base_model_id = 'microsoft/phi-2'\\nmodel = AutoModelForCausalLM.from_pretrained(\"\n",
        "suffix_load_model = \"<fim_suffix>)\\n\"\n",
        "input_text = prefix_load_model + suffix_load_model + '<fim_middle>'\n",
        "\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, **params)\n",
        "print(f'\\033[96m {format_middle_output(tokenizer.decode(outputs[0]))} \\033[00m')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49DroUSiEDAL",
        "outputId": "ab683639-611e-497b-9e78-d8614e668bba"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m base_model_id ='microsoft/phi-2'\n",
            "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
            " \u001b[00m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 2\n",
        "Initialize model using `from_pretrained` with additional comment."
      ],
      "metadata": {
        "id": "lcpbZSTohezn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix_load_model_comment = \"<fim_prefix># Initialize model and set load_in_8bit to True\\nbase_model_id = 'microsoft/phi-2'\\nmodel = AutoModelForCausalLM.from_pretrained(\"\n",
        "suffix_load_model_comment = \"<fim_suffix>)\\n\"\n",
        "input_text = prefix_load_model_comment + suffix_load_model_comment + '<fim_middle>'\n",
        "\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, **params)\n",
        "print(f'\\033[96m {format_middle_output(tokenizer.decode(outputs[0]))} \\033[00m')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2yXqfJZjg1F",
        "outputId": "5c21a799-c142-4e74-e1c8-08b58ac2e01d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m # Initialize model and set load_in_8bit to True\n",
            "base_model_id ='microsoft/phi-2'\n",
            "model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
            " \u001b[00m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 3\n",
        "Tokenize labels within tokenization function."
      ],
      "metadata": {
        "id": "ZpvJ5S2chfs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix_tokenize_labels = \"\"\"<fim_prefix>def tokenize(prompt): \\nresult = tokenizer(prompt['prompt'], max_length=max_input_length, truncation=True, padding=True)\\n\"\"\"\n",
        "suffix_tokenize_labels = \"\"\"<fim_suffix>\\nresult[\"labels\"] = labels[\"input_ids\"] \\n return result\"\"\"\n",
        "input_text = prefix_tokenize_labels + suffix_tokenize_labels + '<fim_middle>'\n",
        "\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, **params)\n",
        "print(f'\\033[96m {format_middle_output(tokenizer.decode(outputs[0]))} \\033[00m')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SIjARhgeqAO",
        "outputId": "154e0e9e-f4e7-4f83-cebb-fdffb9419d90"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m def tokenize(prompt): \n",
            "result = tokenizer(prompt['prompt'], max_length=max_input_length, truncation=True, padding=True)\n",
            "    print(\"Tokens: \", result.keys())\n",
            "result[\"labels\"] = labels[\"input_ids\"] \n",
            " return result \u001b[00m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 4\n",
        "Map tokenization function to the dataset."
      ],
      "metadata": {
        "id": "B8F2kaVhhgfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix_map_tokenization = \"\"\"<fim_prefix>def generate_and_tokenize_prompt(data_point):\\n\\treturn tokenize(data_point)\\ntokenized_train_dataset = dataset.map(\"\"\"\n",
        "suffix_map_tokenization = \"<fim_suffix>)\\n\"\n",
        "input_text = prefix_map_tokenization + suffix_map_tokenization + '<fim_middle>'\n",
        "\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs, pad_token_id=tokenizer.eos_token_id, **params)\n",
        "print(f'\\033[96m {format_middle_output(tokenizer.decode(outputs[0]))} \\033[00m')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARW5EF_gfW86",
        "outputId": "9d0eb203-b128-4805-f5fc-3caf299eca20"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[96m def generate_and_tokenize_prompt(data_point):\n",
            "\treturn tokenize(data_point)\n",
            "tokenized_train_dataset = dataset.map(generate_and_tokenize_prompt, num_parallel_calls=4)\n",
            " \u001b[00m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results analysis\n",
        "We can see that the model did well with simple examples (as were provided by the creator) but generated some artifacts during the test of its own examples:\n",
        "- Example 1: Set `model_name` instead of `base_model_id`.\n",
        "- Example 2: Didn't follow the instructions in the comment and didn't set `load_in_8bit` to True for initialization.\n",
        "- Example 3: Just wrong problem understanding.\n",
        "- Example 4: Did fairly well on task, but set `num_parallel_calls` to 4 without any specific instruction."
      ],
      "metadata": {
        "id": "LwCWIn01g0XM"
      }
    }
  ]
}